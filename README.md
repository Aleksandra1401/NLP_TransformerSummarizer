# NLP_TransformerSummarizer

The content of this repository is proposed solution for implementation of Transformer Summarizer trained on the WikiHow Dataset.

Dataset consists of about 200000 long-sequnece pairs preprocessed for summarization task. Each pair is consists of the documment with matching summary. Each document is the concatenation of the paragraphs of WikiHow article, while summaries are generated by concatenation of paragraph titles. 
Dataset and detailed description can be found at https://github.com/mahnazkoupaee/WikiHow-Dataset

The NLP_TransformerSummarizer_RichardAleksandra.ipnyb is the notebook defining the proposed solutionfor Transformer summarizer implementation. It defines methods to load the dataset, data preprocessing, model design and evaluation approach.
Proposed model is summarizer made of T5 layer based encoder, with multihead attention and transformer layers. Decoder contains 6 Transformer layers. 
Evaluation metrics attempted to be applied are ROUGE-1, ROUGE-2 and ROUGE-L.
